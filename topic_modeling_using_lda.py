# -*- coding: utf-8 -*-
"""Topic Modeling using LDA

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ugUdYxNPZIFRg-K1RA-5IZ0NsfImnycU
"""



!pip install nltk

from sklearn.datasets import fetch_20newsgroups

# Load the dataset
newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))
documents = newsgroups.data

!pip install nltk
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer

# Download necessary NLTK resources
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt') # Download the Punkt sentence tokenizer

# Initialize stop words and lemmatizer
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess(text):
    # Tokenization
    tokens = nltk.word_tokenize(text.lower())
    # Lemmatization and stop words removal
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalpha() and word not in stop_words]
    return ' '.join(tokens)

from gensim import corpora, models

# Preprocess the documents using the function you defined earlier
processed_docs = [preprocess(doc) for doc in documents]

# Tokenize the preprocessed documents
tokenized_docs = [doc.split() for doc in processed_docs]

# Create a dictionary and a corpus
dictionary = corpora.Dictionary(tokenized_docs)
corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]

# Apply LDA
lda_model = models.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=15)

# Display topics
for idx, topic in lda_model.print_topics(-1):
    print('Topic: {} \nWords: {}'.format(idx, topic))

pip install pyLDAvis

import pyLDAvis.gensim

# Prepare visualization data
lda_vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary)
pyLDAvis.display(lda_vis)

pip install gensim

!wget -c "https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz"

!pip install gensim

import gensim.downloader as api

# Load pre-trained GloVe model (e.g., 100-dimensional vectors)
glove_model = api.load('glove-wiki-gigaword-100')

def get_document_embedding_glove(doc, model):
    """Generate a document embedding by averaging word embeddings using GloVe."""
    words = doc.split()
    valid_words = [word for word in words if word in model]  # Filter words present in the model
    if valid_words:
        return np.mean([model[word] for word in valid_words], axis=0)  # Average embeddings of valid words
    else:
        return np.zeros(model.vector_size)  # Return a zero vector if no valid words

# Example usage:
document_embedding = get_document_embedding_glove(processed_docs[0], glove_model)
print(document_embedding)

!pip install gensim

import gensim.downloader as api

# Load pre-trained FastText model
fasttext_model = api.load('fasttext-wiki-news-subwords-300')

def get_document_embedding_fasttext(doc, model):
    """Generate a document embedding by averaging word embeddings using FastText."""
    words = doc.split()
    valid_words = [word for word in words if word in model]  # Filter words present in the model
    if valid_words:
        return np.mean([model[word] for word in valid_words], axis=0)  # Average embeddings of valid words
    else:
        return np.zeros(model.vector_size)  # Return a zero vector if no valid words

# Example usage:
document_embedding = get_document_embedding_fasttext(processed_docs[0], fasttext_model)
print(document_embedding)

def get_lda_topic_distribution(doc, lda_model, dictionary, num_topics):
    bow_vector = dictionary.doc2bow(doc.split())
    lda_vector = lda_model[bow_vector]
    # Initialize a vector of zeros for the full topic space
    topic_distribution = np.zeros(num_topics)
    # Populate the vector with topic probabilities
    for topic_id, prob in lda_vector:
        topic_distribution[topic_id] = prob
    return topic_distribution

# Get the number of topics from the LDA model
num_topics = lda_model.num_topics

# Convert documents to LDA topic vectors
lda_topic_vectors = np.array([get_lda_topic_distribution(doc, lda_model, dictionary, num_topics) for doc in processed_docs])

from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Calculate cosine similarity between LDA topic vectors
lda_similarity_matrix = cosine_similarity(lda_topic_vectors)

# Perform K-Means clustering
num_clusters = 5  # Adjust the number of clusters as needed
kmeans = KMeans(n_clusters=num_clusters, random_state=42)
lda_labels = kmeans.fit_predict(lda_topic_vectors)

# Visualize clusters using PCA
pca = PCA(n_components=2)
lda_reduced = pca.fit_transform(lda_topic_vectors)

plt.figure(figsize=(10, 8))
plt.scatter(lda_reduced[:, 0], lda_reduced[:, 1], c=lda_labels, cmap='viridis')
plt.title('Document Clusters Based on LDA Topic Distributions')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.colorbar()
plt.show()